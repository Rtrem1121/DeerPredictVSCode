#!/usr/bin/env python3
"""
Test Deduplication Tool - Phase 3
Analyzes and consolidates duplicate test names
"""

import json
import ast
from pathlib import Path
from typing import Dict, List, Set
from collections import defaultdict

class TestDeduplicator:
    def __init__(self, audit_report_path: str):
        with open(audit_report_path, 'r') as f:
            self.report = json.load(f)
        
        # Extract examples from duplicates section
        self.duplicates = self.report['duplicates'].get('examples', {})
        self.recommendations = []
        self.archive_candidates = []
    
    def analyze_duplicate_group(self, test_name: str, file_list: List[str]) -> Dict:
        """Analyze a group of duplicate tests"""
        print(f"\n📊 Analyzing: {test_name}")
        print(f"   Found in {len(file_list)} files")
        
        # Get file details
        file_details = []
        for file_path in file_list:
            # Find file in audit report
            for file_info in self.report['files']:
                if file_info['relative_path'] == file_path:
                    file_details.append(file_info)
                    break
        
        # Score each file
        scores = []
        for detail in file_details:
            score = self._score_file(detail)
            scores.append({
                'file': detail['relative_path'],
                'score': score,
                'test_count': detail.get('test_count', 0),
                'location': detail['location_category'],
                'category': detail['name_category']
            })
        
        # Sort by score (highest first)
        scores.sort(key=lambda x: x['score'], reverse=True)
        
        # Recommendation
        best_file = scores[0]
        archive_files = [s['file'] for s in scores[1:]]
        
        return {
            'test_name': test_name,
            'total_files': len(file_list),
            'recommended_keep': best_file['file'],
            'recommended_archive': archive_files,
            'scores': scores
        }
    
    def _score_file(self, file_info: Dict) -> int:
        """Score a file's quality (higher = better)"""
        score = 0
        
        # Prefer organized files
        if 'organized' in file_info['location_category']:
            score += 50
        
        # Prefer files with more tests (comprehensive)
        score += min(file_info.get('test_count', 0) * 5, 30)
        
        # Prefer integration tests over unit tests for duplicates
        if file_info['test_type'] == 'integration':
            score += 15
        elif file_info['test_type'] == 'unit':
            score += 10
        
        # Prefer non-archived files
        if file_info['location_category'] != 'archived':
            score += 20
        
        # Prefer files in specific categories
        valuable_categories = ['integration', 'backend', 'frontend', 'validation']
        if file_info['name_category'] in valuable_categories:
            score += 10
        
        return score
    
    def analyze_all_duplicates(self):
        """Analyze all duplicate test groups"""
        print("=" * 70)
        print("🔍 DUPLICATE TEST ANALYSIS")
        print("=" * 70)
        
        for test_name, file_list in self.duplicates.items():
            recommendation = self.analyze_duplicate_group(test_name, file_list)
            self.recommendations.append(recommendation)
            
            # Collect archive candidates
            self.archive_candidates.extend(recommendation['recommended_archive'])
    
    def generate_consolidation_plan(self):
        """Generate consolidation plan"""
        print()
        print("=" * 70)
        print("📋 CONSOLIDATION PLAN")
        print("=" * 70)
        print()
        
        print(f"📊 Total Duplicate Groups: {len(self.recommendations)}")
        print(f"📁 Files to Keep: {len(self.recommendations)}")
        print(f"🗂️  Files to Archive: {len(set(self.archive_candidates))}")
        print()
        
        print("🎯 Top Priority Consolidations:")
        print()
        
        # Sort by total files (worst offenders first)
        sorted_recs = sorted(
            self.recommendations,
            key=lambda x: x['total_files'],
            reverse=True
        )
        
        for i, rec in enumerate(sorted_recs[:10], 1):
            print(f"{i}. {rec['test_name']}")
            print(f"   📁 {rec['total_files']} duplicates")
            print(f"   ✅ Keep: {Path(rec['recommended_keep']).name}")
            print(f"   🗂️  Archive: {len(rec['recommended_archive'])} files")
            print()
    
    def generate_deduplication_script(self):
        """Generate shell script to execute deduplication"""
        script_lines = [
            "# Deduplication Script - Generated by Test Deduplicator",
            "# Moves duplicate test files to archive",
            "",
            "# Create archive directory",
            "New-Item -ItemType Directory -Force -Path archive/tests/duplicates_$(Get-Date -Format 'yyyyMMdd')",
            ""
        ]
        
        # Get unique archive candidates
        unique_archives = set(self.archive_candidates)
        
        for file_path in sorted(unique_archives):
            # Skip if file is already archived
            if 'archive' in file_path or 'dead_code' in file_path:
                continue
            
            filename = Path(file_path).name
            script_lines.append(
                f"# Archive {filename}"
            )
            script_lines.append(
                f"Move-Item '{file_path}' 'archive/tests/duplicates_$(Get-Date -Format \"yyyyMMdd\")/{filename}' -Force"
            )
            script_lines.append("")
        
        # Save script
        script_path = Path('scripts/deduplicate_tests.ps1')
        script_path.write_text('\n'.join(script_lines), encoding='utf-8')
        
        print(f"💾 Deduplication script saved: {script_path}")
        print()
    
    def save_report(self):
        """Save deduplication report"""
        report = {
            'timestamp': '2025-10-02',
            'total_duplicate_groups': len(self.recommendations),
            'files_to_keep': len(self.recommendations),
            'files_to_archive': len(set(self.archive_candidates)),
            'recommendations': self.recommendations
        }
        
        report_path = Path('deduplication_report.json')
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"💾 Deduplication report saved: {report_path}")


def main():
    """Main execution"""
    print("🚀 Phase 3: Test Deduplication Tool")
    print()
    
    # Load audit report
    audit_file = 'test_audit_report.json'
    if not Path(audit_file).exists():
        print(f"❌ Error: {audit_file} not found")
        return 1
    
    deduplicator = TestDeduplicator(audit_file)
    
    # Analyze duplicates
    deduplicator.analyze_all_duplicates()
    
    # Generate plan
    deduplicator.generate_consolidation_plan()
    
    # Generate script
    deduplicator.generate_deduplication_script()
    
    # Save report
    deduplicator.save_report()
    
    print("=" * 70)
    print("✅ DEDUPLICATION ANALYSIS COMPLETE")
    print("=" * 70)
    print()
    print("📝 Next Steps:")
    print("  1. Review deduplication_report.json")
    print("  2. Manually verify recommended consolidations")
    print("  3. Run scripts/deduplicate_tests.ps1 (OPTIONAL)")
    print("  4. Continue with Phase 3 subdirectory organization")
    print()
    
    return 0


if __name__ == '__main__':
    exit(main())
